# Awesome-crawler-cn
互联网爬虫，蜘蛛，数据采集器，网页解析器的汇总，因新技术不断发展，新框架层出不穷，此文会不断更新...

## 交流讨论
1. 欢迎推荐你知道的开源网络爬虫，网页抽取框架.
2. 开源网络爬虫QQ交流群:322937592
3. email address: liinux at qq.com

## Python 
* [Scrapy](https://github.com/scrapy/scrapy) - 一种高效的屏幕,网页数据采集框架。
    * [django-dynamic-scraper](https://github.com/holgerd77/django-dynamic-scraper) - 基于Scrapy内核由django Web框架开发的爬虫。
    * [Scrapy-Redis](https://github.com/rolando/scrapy-redis) - 基于Scrapy内核采用Redis组件的爬虫。
    * [scrapy-cluster](https://github.com/istresearch/scrapy-cluster) - 基于Scrapy内核采用Redis 和 Kafka 开发的分布式爬虫框架。
    * [distribute_crawler](https://github.com/gnemoug/distribute_crawler) - 基于Scrapy内核采用redis, mongodb开发的分布式爬虫框架。
* [pyspider](https://github.com/binux/pyspider) - 一个强大纯python的数据采集系统.
* [cola](https://github.com/chineking/cola) - 一个分布式的爬虫框架.
* [Demiurge](https://github.com/matiasb/demiurge) - 基于PyQuery的微型爬虫框架.
* [Scrapely](https://github.com/scrapy/scrapely) - 一个纯python的HTML页面捕捉库.
* [feedparser](http://pythonhosted.org/feedparser/) - 一个通用的feed解析器.
* [you-get](https://github.com/soimort/you-get) -  静默网站爬去下载器.
* [Grab](http://grablib.org/) - 网站采集框架.
* [MechanicalSoup](https://github.com/hickford/MechanicalSoup) - 一个自动化的互动网站Python库.
* [portia](https://github.com/scrapinghub/portia) - 基于Scrapy的可视化数据采集框架.
* [crawley](https://github.com/jmg/crawley) - 基于非阻塞通信(NIO)的python爬虫框架.
* [RoboBrowser](https://github.com/jmcarp/robobrowser) - 一个简单的，不基于Web浏览器的基于Python的Web 浏览器.
* [MSpider](https://github.com/manning23/MSpider) - 一个基于gevent(协程网络库)的python爬虫. 
* [brownant](https://github.com/douban/brownant) - 一个轻量级的网络数据抽取框架.

## Java
* [Apache Nutch](http://nutch.apache.org/) - 用于生产环境的高度可扩展的高度可扩展的网络爬虫.
   * [anthelion](https://github.com/yahoo/anthelion) - 一个基于Apache Nutch抓取语义注释在HTML页面插件.
* [Crawler4j](https://github.com/yasserg/crawler4j) - 简单和轻量级的网络爬虫.
* [JSoup](http://jsoup.org/) - 采集，分析，处理和清洗HTML页面.
* [websphinx](http://www.cs.cmu.edu/~rcm/websphinx/) - HTML网站特定的处理、信息提取.
* [Open Search Server](http://www.opensearchserver.com/) - 全套搜索功能，建立你自己的索引策略。分析、提取全文数据，这个框架可以索引的一切.
* [Gecco](https://github.com/xtuhcy/gecco) - 一个易于使用的轻量级网络爬虫.
* [WebCollector](https://github.com/CrawlScript/WebCollector) -简单的抓取网页的界面，可以在不到5分钟内部署一个多线程的网络爬虫.
* [Webmagic](https://github.com/code4craft/webmagic) -一个可扩展的爬虫框架.
* [Spiderman](https://git.oschina.net/l-weiwei/spiderman) -一个可扩展的，多线程的网络爬虫.
    * [Spiderman2](http://git.oschina.net/l-weiwei/Spiderman2) - 分布式网络爬虫框架，支持javascript渲染.
* [Heritrix3](https://github.com/internetarchive/heritrix3) -  可扩展，大规模的网络爬虫项目.
* [SeimiCrawler](https://github.com/zhegexiaohuozi/SeimiCrawler) - 一个敏捷的分布式爬虫框架.
* [StormCrawler](http://github.com/DigitalPebble/storm-crawler/) - 基于开放源代码、构建低延迟的网络资源采集框架，基于Apache Storm.
* [Spark-Crawler](https://github.com/USCDataScience/sparkler) - 基于Apache Nutch 的网络爬虫，可以运行于Spark.

## C# 
* [ccrawler](http://www.findbestopensource.com/product/ccrawler) - Built in C# 3.5 version. it contains a simple extention of web content categorizer, which can saparate between the web page depending on their content.
* [SimpleCrawler](https://github.com/lei-zhu/SimpleCrawler) - Simple spider base on mutithreading, regluar expression.
* [DotnetSpider](https://github.com/zlzforever/DotnetSpider) - This is a cross platfrom, ligth spider develop by C#.
* [Abot](https://github.com/sjdirect/abot) - C# web crawler built for speed and flexibility.
* [Hawk](https://github.com/ferventdesert/Hawk) - Advanced Crawler and ETL tool written in C#/WPF.
* [SkyScraper](https://github.com/JonCanning/SkyScraper) - An asynchronous web scraper / web crawler using async / await and Reactive Extensions.

## JavaScript
* [scraperjs](https://github.com/ruipgil/scraperjs) - A complete and versatile web scraper.
* [scrape-it](https://github.com/IonicaBizau/scrape-it) - A Node.js scraper for humans.
* [simplecrawler](https://github.com/cgiffard/node-simplecrawler) - Event driven web crawler.
* [node-crawler](https://github.com/bda-research/node-crawler) - Node-crawler has clean,simple api.
* [js-crawler](https://github.com/antivanov/js-crawler) - Web crawler for Node.JS, both HTTP and HTTPS are supported.
* [x-ray](https://github.com/lapwinglabs/x-ray) - Web scraper with pagination and crawler support.
* [node-osmosis](https://github.com/rchipka/node-osmosis) - HTML/XML parser and web scraper for Node.js.

## PHP
* [Goutte](https://github.com/FriendsOfPHP/Goutte) - A screen scraping and web crawling library for PHP.
    * [laravel-goutte](https://github.com/dweidner/laravel-goutte) - Laravel 5 Facade for Goutte.
* [dom-crawler](https://github.com/symfony/dom-crawler) - The DomCrawler component eases DOM navigation for HTML and XML documents.
* [pspider](https://github.com/hightman/pspider) - Parallel web crawler written in PHP.
* [php-spider](https://github.com/mvdbos/php-spider) - A configurable and extensible PHP web spider.

## C++
* [open-source-search-engine](https://github.com/gigablast/open-source-search-engine) - A distributed open source search engine and spider/crawler written in C/C++.

## C
* [httrack](https://github.com/xroche/httrack) - Copy websites to your computer.

## Ruby
* [upton](https://github.com/propublica/upton) - A batteries-included framework for easy web-scraping. Just add CSS(Or do more).
* [wombat](https://github.com/felipecsl/wombat) - Lightweight Ruby web crawler/scraper with an elegant DSL which extracts structured data from pages.
* [RubyRetriever](https://github.com/joenorton/rubyretriever) - RubyRetriever is a Web Crawler, Scraper & File Harvester.
* [Spidr](https://github.com/postmodern/spidr) - Spider a site ,multiple domains, certain links or infinitely.
* [Cobweb](https://github.com/stewartmckee/cobweb) - Web crawler with very flexible crawling options, standalone or using sidekiq.
* [mechanize](https://github.com/sparklemotion/mechanize) - Automated web interaction & crawling.

## R
* [rvest](https://github.com/hadley/rvest) - Simple web scraping for R.

## Erlang 
* [ebot](https://github.com/matteoredaelli/ebot) - A scalable, distribuited and highly configurable web cawler.

## Perl
* [web-scraper](https://github.com/miyagawa/web-scraper) - Web Scraping Toolkit using HTML and CSS Selectors or XPath expressions.

## Go
* [pholcus](https://github.com/henrylee2cn/pholcus) -  A distributed, high concurrency and powerful web crawler.
* [gocrawl](https://github.com/PuerkitoBio/gocrawl) - Polite, slim and concurrent web crawler.
* [fetchbot](https://github.com/PuerkitoBio/fetchbot) - A simple and flexible web crawler that follows the robots.txt policies and crawl delays.
* [go_spider](https://github.com/hu17889/go_spider) - An awesome Go concurrent Crawler(spider) framework. 
* [dht](https://github.com/shiyanhui/dht) - BitTorrent DHT Protocol && DHT Spider.
* [ants-go](https://github.com/wcong/ants-go) - A open source, distributed, restful crawler engine in golang.
* [scrape](https://github.com/yhat/scrape) - A simple, higher level interface for Go web scraping.

## Scala
* [crawler](https://github.com/bplawler/crawler) - Scala DSL for web crawling.
* [scrala](https://github.com/gaocegege/scrala) - Scala crawler(spider) framework, inspired by scrapy.
* [ferrit](https://github.com/reggoodwin/ferrit) - Ferrit is a web crawler service written in Scala using Akka, Spray and Cassandra.
